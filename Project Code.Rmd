---
output: 
  pdf_document :
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(mgcv)
library(earth)
library(gbm)
library(corrplot)
library(gridExtra)
library(kernlab)
library(klaR)
library(rpart.plot)

knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = 'center',
  out.width = "65%",
  strip.white = TRUE,
  warning = FALSE)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Secondary Analysis -- Figures and outputs

```{r}
# load the external dataset 
load("recovery.rdata")
# change the variable type based on the reference
index_factor = c(3, 4, 5,  9, 10, 13, 14, 15)
index_numer = c(2, 6, 7, 8, 11, 12)
dat[, index_factor]= lapply(dat[, index_factor], as.factor)
# extract 2000+2000 samples for analysis
set.seed(2604)
dat_1 <- dat[sample(1:10000, 2000),]
set.seed(3508)
dat_2 <- dat[sample(1:10000, 2000),]
# merge the dataset for unique values
# create a new variable length_ind with 30 days as threshold 
dat = rbind(dat_1, dat_2) %>% 
  unique() %>% 
  mutate(
    length_ind = ifelse(recovery_time>30, "yes", "no"),
    length_ind = as.factor(length_ind)
  ) 
dat$length_ind = relevel(dat$length_ind, ref = "no")

# seperate training set and test set
set.seed(2023)
train_row = createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)
```

```{r train/test split for secondary analysis}
# create covariates matrix for training and test
predictors_train_new = model.matrix(length_ind ~ ., data = dat[train_row, -c(1,16)])[, -1]
predictors_test_new = model.matrix(length_ind ~ ., data = dat[-train_row, -c(1,16)])[, -1]
# create response vector for training and test
response_train_new = dat[train_row, -c(1,16)]$length_ind
response_test_new = dat[-train_row, -c(1,16)]$length_ind
```

### Exploratory analysis for classification

```{r}
# summary statistics
dat %>% 
  skimr::skim() %>% 
  knitr::knit_print()
# LDA plots
klaR::partimat(length_ind ~ age+bmi+SBP+LDL,
         data = dat, method = "lda")
```

### Penalized logistic regression

```{r}
ctrl_2 = trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
set.seed(2)

glmnGrid = expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-10, -2, length = 20)))

glmn_model <- train(x = predictors_train_new,
                    y = response_train_new,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl_2)

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(glmn_model, par.settings = myPar, xTrans = function(x) log(x))
```

### Multivariate adaptive regression splines classifier (MARS)

```{r}
set.seed(2)
mars_model = train(x = predictors_train_new,
                   y = response_train_new,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3,nprune = 2:20),
                   metric = "ROC",
                   trControl = ctrl_2)
plot(mars_model)
```

### Quadratic discriminant analysis (QDA)

```{r}
set.seed(2)
qda_model = train(x = predictors_train_new,
                   y = response_train_new,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl_2)
```

### Naive Bayes Classifer

```{r}
nbGrid = expand.grid(usekernel = c(FALSE,TRUE),
                     fL = 1,
                     adjust = seq(.2, 3, by = .2))
set.seed(2)
nb_model = train(x = predictors_train_new,
                 y = response_train_new,
                 method = "nb",
                 tuneGrid = nbGrid,
                 metric = "ROC",
                 trControl = ctrl_2)
plot(nb_model)
```

### Classification Tree

```{r}
set.seed(2)
rpart_model = train(predictors_train_new, response_train_new,
                    method = "rpart",
                    tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                    trControl = ctrl_2,
                    metric = "ROC")
ggplot(rpart_model, highlight = TRUE)
```

### Adaptive boosting classfier

```{r cache=TRUE}
# set tunning parameters
# learning rate selection criterion : max(0.01, 0.1*(min(1, nl/10000)))
gbmA_grid = expand.grid(n.trees = c(seq(100, 1200, by = 100)),
                         interaction.depth = 1:3,
                         shrinkage = c(0.01, 0.03),
                         n.minobsinnode = 5)
set.seed(2)
gbmA_model = train(predictors_train_new, response_train_new,
                  tuneGrid = gbmA_grid,
                  trControl = ctrl_2,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)
ggplot(gbmA_model, highlight = TRUE)
```

### Support vector classifier with linear kernel

```{r results='hide',fig.keep='all', cache=TRUE}
set.seed(2)
svml_model = train(predictors_train_new, response_train_new,
                   method = "svmLinear",
                   # setting tunning parameters
                   tuneGrid = data.frame(C = exp(seq(-5,2,len=50))),
                   trControl = ctrl_2,
                   verbose = FALSE)
plot(svml_model, highlight = TRUE, xTrans = log)
```

### Support vector classifier with radial kernel

```{r results='hide',fig.keep='all', cache=TRUE}
# setting tunning parameters
svmr_grid = expand.grid(C = exp(seq(-5,2,len=20)),
                        sigma = exp(seq(-8,-3,len=10)))
# tunes over both cost and sigma
set.seed(2)
svmr_model = train(predictors_train_new, response_train_new,
                   method = "svmRadialSigma",
                   tuneGrid = svmr_grid,
                   trControl = ctrl_2,
                   verbose = FALSE)
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))
ggplot(svmr_model, highlight = TRUE, par.settings = myPar)

```

# Figures and Tables for Secondary Analysis 
## Models Comparsion based on Area Under Curves

```{r}
resamp_2 = resamples(list(
  glmn = glmn_model,
  mars = mars_model,
  qda = qda_model,
  nb = nb_model,
  svmr = svmr_model,
  svml = svml_model,
  gbmA = gbmA_model,
  rpart = rpart_model
))
bwplot(resamp_2, metric = "ROC")

```

## Final model performance on test dataset

```{r}
# find test Accurancy of the final model
postResample(predict(gbmA_model, predictors_test_new),
             response_test_new) %>% knitr::kable()
```

### Variable importance plots of the final model

```{r}
par(mfrow = c(1, 1))
var_df_2 = summary(gbmA_model,
        cBars = 10,
        las = 2)  
var_df_2 %>% 
  as.data.frame() %>% 
  select(-var) %>% 
  knitr::kable()
```

### Final Model intepretation

```{r}
pdp::partial(gbmA_model, pred.var = c("bmi"),
             grid.resolution = 10, type = "classification", which.class = "yes", prob = TRUE,) %>% 
  autoplot(train = predictors_train_new, rug = TRUE, 
           main = "Partial dependence plot by BMI") +
  theme(plot.title = element_text(hjust = 0.5))

pdp::partial(gbmA_model, pred.var = c("SBP","LDL"),
             chull = TRUE,
             grid.resolution = 10, type = "classification", which.class = "yes", prob = TRUE) %>% 
  autoplot(train = predictors_train_new, rug = TRUE,legend.title = "Probability of long recovery time",
           main = "Partial dependence plot by SBP and LDL") +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r}
gbmA_model %>%
  pdp::partial(pred.var = "bmi",
          grid.resolution = 100,
          ice = TRUE, type = "classification", which.class = "yes", prob = TRUE) %>%
  autoplot(train = predictors_train_new, alpha = .1) +
  ggtitle("ICE plot for bmi versus probability of long recovery (>30)") +
  theme(plot.title = element_text(hjust = 0.5))
```

# Appendix -- Primary Analysis of Covid Recovery Time


```{r train/test split for primary analysis}
# create covariates matrix for training and test
predictors_train = model.matrix(recovery_time ~ ., data = dat[train_row, -c(1,17)])[, -1]
predictors_test = model.matrix(recovery_time ~ ., data = dat[-train_row, -c(1,17)])[, -1]
# create response vector for training and test
response_train = dat[train_row, -c(1,17)]$recovery_time
response_test = dat[-train_row, -c(1,17)]$recovery_time
```

```{r train/test split for secondary analysis}
# create covariates matrix for training and test
predictors_train_new = model.matrix(length_ind ~ ., data = dat[train_row, -c(1,16)])[, -1]
predictors_test_new = model.matrix(length_ind ~ ., data = dat[-train_row, -c(1,16)])[, -1]
# create response vector for training and test
response_train_new = dat[train_row, -c(1,16)]$length_ind
response_test_new = dat[-train_row, -c(1,16)]$length_ind
```

## Exploratory analysis and data visualization

```{r}
table(dat$length_ind, dat$study) %>% barplot(main = "Number of cases seperated by 30 days in recovery by Study Groups",
                                             xlab = "Class",
                                             col = c("White","Black"))
legend("topright",
       c("<= 30 days","> 30 days"),
       fill = c("White","Black"),
       cex = 0.7)
```

### Visualize potential relationship between reponse variable and numeric predictors

```{r}
# simple visualization of the numeric data
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x = dat[, index_numer],
            y = dat$recovery_time, 
            plot = "scatter", 
            type = c("p", "smooth"),
            layout = c(3, 2))

```

### Visualize potential relationship between reponse variable and categorical predictors

```{r}
# simple visualization of the categorical data
par(mfrow=c(2,4))
myColors = c(rgb(0.1,0.1,0.7,0.5) , rgb(0.8,0.1,0.3,0.6), rgb(0.8,0.8,0.3,0.5),
             rgb(0.4,0.2,0.3,0.6))

boxplot(recovery_time ~ gender, data = dat, xlab = "gender", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ race, data = dat, xlab = "race", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ smoking, data = dat, xlab = "smoking status", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ hypertension, data = dat, xlab = "hypertension", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ diabetes, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ vaccine, data = dat, xlab = "vaccine", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ severity, data = dat, xlab = "diabetes", col = myColors, ylim=c(0, 150))
boxplot(recovery_time ~ study, data = dat, xlab = "study", col = myColors, ylim=c(0, 150))
```

### Correlation plot to check collinearity between covariates (based on training data) 

```{r}
cor(predictors_train[, c(1, 8, 9, 10, 13, 14)]) %>% corrplot(
  method = "circle", type = "full", 
  addCoef.col = 1, number.font =0.5,
  tl.col="black", tl.srt=90, tl.cex = 0.5,
  insig = "blank", diag=FALSE, number.cex = .3)
```

## Model Training

### Ordinary Least square

```{r}
# set train method
ctrl_1 = trainControl(method = "repeatedcv", number = 10, repeats = 5)
# build the linear least squared model with caret
set.seed(1)
lm_model = train(predictors_train, response_train, method = "lm", trControl = ctrl_1)
par(mfrow = c(2, 2))
plot(lm_model$finalModel)
```

### Elastic net regression

```{r}
set.seed(1)
# build elastic net model with caret
elnet_model = train(predictors_train, response_train, 
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = seq(0, 1, length=21),
                                           lambda = exp(seq(-2, 8, length=50))),
                    trControl = ctrl_1)
myCol<- rainbow(21)
myPar <- list(superpose.symbol = list(col = myCol),
superpose.line = list(col = myCol))
plot(elnet_model, par.settings = myPar)
# show the best lambda and alpha combination with lowest cv rmse
elnet_model$bestTune
```

### Partial least squares (PLS)

```{r}
set.seed(1)
pls_model = train(predictors_train, response_train,
                method = "pls",
                # 18 variables in total
                tuneGrid = data.frame(ncomp = 1:18), 
                trControl = ctrl_1,
                preProcess = c("center", "scale"))
ggplot(pls_model, highlight = TRUE) + 
  scale_x_continuous(breaks = seq(0,20,by=1))
```

### Generalized Additive Models (GAM)

```{r}
set.seed(1)
tune_Grid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE))
gam_model = train(predictors_train, response_train, 
                   method = "gam",
                  tuneGrid = tune_Grid,
                  trControl = ctrl_1)
par(mfrow = c(3, 2))
plot(gam_model$finalModel, shade = TRUE)
```

### Multivariate adaptive regression spline model (MARS)

```{r}
set.seed(1)
# Set tuning parameters (18 as maximum number of terms taken since only 18 predictors are used)
mars_grid = expand.grid(degree = 1:3, nprune = 2:18)
# Fit MARS model
mars_model = train(predictors_train, response_train, 
                   method = "earth",
                   tuneGrid = mars_grid,
                   trControl = ctrl_1)
# Plot the model
plot(mars_model)
summary(mars_model)
```

### K-Nearest Neighbors (KNN)

```{r}
set.seed(1)
knn_grid = expand.grid(k = seq(5, 15, by = 1))
knn_model = train(predictors_train, response_train,
                  method = "knn",
                  preProcess = c("center", "scale"),
                  trControl = ctrl_1,
                  tuneGrid = knn_grid)
knn_model$bestTune
```

### Regression Tree (CART)

```{r}
set.seed(1)

# build a regression tree on the training data using the caret package
rpart_model <- train(recovery_time ~ . ,
                   data = dat[train_row, -c(1,17)], # training data
                   method = "rpart", # regression tree model
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))), # candidate values for the cp that controls pruning
                   trControl = ctrl_1)

# create a plot of the complexity parameter selection
ggplot(rpart_model, highlight = TRUE) # highlight the optimal cp value
rpart_model$bestTune

# create a plot of the tree using the rpart.plot() function
rpart.plot(rpart_model$finalModel)

```

### Random Forest

```{r}
set.seed(1)
# build a random forest model
# mtry: Since mtry should be set to the square root of the total number of predictors for classification problems, and the total number of predictors divided by three. With 18 predictors, the optimal tuning range for mtry would be between 4 and 6.
 
rf_grid <- expand.grid(mtry = 4:6,
                       splitrule = "variance",
                       min.node.size = 3:8)

rf_model <- train(recovery_time ~ .,
                dat[train_row, -c(1,17)],
                method = "ranger",
                tuneGrid = rf_grid,
                trControl = ctrl_1)

# create a plot
ggplot(rf_model, highlight = TRUE)
```

### Generalized Boosted Regression

```{r}
set.seed(1)
# setting number of trees, depth, learning rate, and default leaves number
# learning rate = max(0.01, 0.1*(min(1, nl/10000)))
gbm_grid = expand.grid(
  n.trees = c(seq(100, 1200, by = 100)),
  interaction.depth = c(1, 2, 3), 
  shrinkage = c(0.01, 0.03),
  n.minobsinnode = 5
)
gbm_model = train(predictors_train, response_train,
                  method = "gbm",
                 preProcess = c("center", "scale"),
                 trControl = ctrl_1,
                 tuneGrid = gbm_grid,
                 verbose = FALSE)
plot(gbm_model)
```

### Models comparsion based on cross validation error

```{r}
# compare model performance through sampling method
resamp = resamples(list(
  lm = lm_model,
  enet = elnet_model,
  pls = pls_model,
  gam = gam_model,
  mars = mars_model,
  knn = knn_model,
  gbm = gbm_model,
  cart = rpart_model,
  rf = rf_model
))

# Summary 
summary(resamp)
# plot resampling rmse
parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")

```

## Results

### Test Mean Squared Error
```{r}
# get test mse
predict_value = predict(gbm_model, newdata = predictors_test)
test_mse = mean((predict_value - response_test)^2)
test_mse %>% knitr::knit_print()
```

### Variable importance plots 

```{r}
par(mfrow = c(1, 1))
var_df = summary(gbm_model,
        cBars = 10,
        las = 2)  

var_df %>% 
  as.data.frame() %>% 
  dplyr::select(-var) %>% 
  knitr::kable()
```

### Partial dependance plots

```{r}
p1 = pdp::partial(gbm_model, pred.var = c("bmi"),
             grid.resolution = 10) %>% 
  autoplot(train = predictors_train, rug = TRUE, 
           main = "Partial dependence plot by BMI") +
  theme(plot.title = element_text(hjust = 0.5))
p2 = pdp::partial(gbm_model, pred.var = c("vaccine1"), 
                  grid.resolution = 10) %>% 
  autoplot(train = predictors_train, rug = TRUE, 
           main = "Partial dependence plot by Vaccine1") +
  theme(plot.title = element_text(hjust = 0.5))
p3 = pdp::partial(gbm_model, pred.var = c("severity1"), 
                  grid.resolution = 10) %>% 
    autoplot(train = predictors_train, rug = TRUE, 
           main = "Partial dependence plot by Severity1") +
  theme(plot.title = element_text(hjust = 0.5))
p4 = pdp::partial(gbm_model, pred.var = c("age"), 
                  grid.resolution = 10) %>% 
    autoplot(train = predictors_train, rug = TRUE, 
           main = "Partial dependence plot by Age") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(arrangeGrob(p1, p4, ncol = 2), arrangeGrob(p2, p3, ncol = 2))
```

### Individual Conditional Expectation (ICE) plot

```{r}
gbm_model %>%
  pdp::partial(pred.var = "bmi",
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = predictors_train, alpha = .1) +
  ggtitle("ICE plot for recovery time versus bmi") +
  theme(plot.title = element_text(hjust = 0.5))
```
