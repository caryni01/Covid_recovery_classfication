---
title: "Covid Recovery Analysis - Secondary Analysis"
output: 
  pdf_document :
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(mgcv)
library(earth)
library(gbm)
library(corrplot)
library(gridExtra)
library(kernlab)


knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = 'center',
  out.width = "70%",
  strip.white = TRUE,
  warning = FALSE)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r}
# load the external dataset 
load("recovery.rdata")
# change the variable type based on the reference
index_factor = c(3, 4, 5,  9, 10, 13, 14, 15)
index_numer = c(2, 6, 7, 8, 11, 12)
dat[, index_factor]= lapply(dat[, index_factor], as.factor)
# extract 2000+2000 samples for analysis
set.seed(2604)
dat_1 <- dat[sample(1:10000, 2000),]
set.seed(3508)
dat_2 <- dat[sample(1:10000, 2000),]
# merge the dataset for unique values
# create a new variable length_ind with 30 days as threshold 
dat = rbind(dat_1, dat_2) %>% 
  unique() %>% 
  mutate(
    length_ind = ifelse(recovery_time>30, "yes", "no"),
    length_ind = as.factor(length_ind)
  ) 

# seperate training set and test set
set.seed(2023)
train_row = createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)
```

```{r train/test split for secondary analysis}
# create covariates matrix for training and test
predictors_train_new = model.matrix(length_ind ~ ., data = dat[train_row, -c(1,16)])[, -1]
predictors_test_new = model.matrix(length_ind ~ ., data = dat[-train_row, -c(1,16)])[, -1]
# create response vector for training and test
response_train_new = dat[train_row, -c(1,16)]$length_ind
response_test_new = dat[-train_row, -c(1,16)]$length_ind
```

### Penalized logistic regression

```{r}
ctrl_2 = trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
set.seed(2)
glmnGrid = expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-10, -2, length = 20)))

glmn_model <- train(x = predictors_train_new,
                    y = response_train_new,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl_2)

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(glmn_model, par.settings = myPar, xTrans = function(x) log(x))
```

### Multivariate adaptive regression splines classifier (MARS)

```{r}
set.seed(2)
mars_model = train(x = predictors_train_new,
                   y = response_train_new,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3,nprune = 2:20),
                   metric = "ROC",
                   trControl = ctrl_2)
plot(mars_model)
```

### Quadratic discriminant analysis (QDA)

```{r}
set.seed(2)
qda_model = train(x = predictors_train_new,
                   y = response_train_new,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl_2)
```

### Naive Bayes Classifer

```{r}
nbGrid = expand.grid(usekernel = c(FALSE,TRUE),
                     fL = 1,
                     adjust = seq(.2, 3, by = .2))
set.seed(2)
nb_model = train(x = predictors_train_new,
                 y = response_train_new,
                 method = "nb",
                 tuneGrid = nbGrid,
                 metric = "ROC",
                 trControl = ctrl_2)
plot(nb_model)
```

### Classification Tree

```{r}
set.seed(2)
rpart_model = train(predictors_train_new, response_train_new,
                    method = "rpart",
                    tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                    trControl = ctrl_2,
                    metric = "ROC")
ggplot(rpart_model, highlight = TRUE)
```

### Adaptive boosting classfier

```{r}
# set tunning parameters
# learning rate selection criterion : max(0.01, 0.1*(min(1, nl/10000)))
gbmA_grid = expand.grid(n.trees = c(seq(100, 1200, by = 100)),
                         interaction.depth = 1:3,
                         shrinkage = c(0.01, 0.03),
                         n.minobsinnode = 5)
set.seed(2)
gbmA_model = train(predictors_train_new, response_train_new,
                  tuneGrid = gbmA_grid,
                  trControl = ctrl_2,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)
ggplot(gbmA_model, highlight = TRUE)
```

### Support vector classifier with linear kernel

```{r results='hide',fig.keep='all'}
set.seed(2)
svml_model = train(predictors_train_new, response_train_new,
                   method = "svmLinear",
                   # setting tunning parameters
                   tuneGrid = data.frame(C = exp(seq(-5,2,len=50))),
                   trControl = ctrl_2,
                   verbose = FALSE)
plot(svml_model, highlight = TRUE, xTrans = log)
```

### Support vector classifier with radial kernel

```{r results='hide',fig.keep='all', cache=TRUE}
# setting tunning parameters
svmr_grid = expand.grid(C = exp(seq(-5,2,len=20)),
                        sigma = exp(seq(-8,-3,len=10)))
# tunes over both cost and sigma
set.seed(2)
svmr_model = train(predictors_train_new, response_train_new,
                   method = "svmRadialSigma",
                   tuneGrid = svmr_grid,
                   trControl = ctrl_2,
                   verbose = FALSE)
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))
ggplot(svmr_model, highlight = TRUE, par.settings = myPar)
```

## Models Comparsion based on Area Under Curves

```{r}
resamp = resamples(list(
  glmn = glmn_model,
  mars = mars_model,
  qda = qda_model,
  nb = nb_model,
  svmr = svmr_model,
  svml = svml_model,
  gbmA = gbmA_model,
  rpart = rpart_model
))
bwplot(resamp, metric = "ROC")
```

## Final model performance on test dataset

```{r}
# find test Accurancy of the final model
postResample(predict(gbmA_model, predictors_test_new),
             response_test_new) %>% knitr::kable()
```

### Variable importance plots of the final model

```{r}
par(mfrow = c(1, 1))
var_df_2 = summary(gbmA_model,
        cBars = 10,
        las = 2)  
var_df_2 %>% 
  as.data.frame() %>% 
  select(-var) %>% 
  knitr::kable()
```

### Final Model intepretation

```{r}
pdp::partial(gbmA_model, pred.var = c("bmi"),
             grid.resolution = 10) %>% 
  autoplot(train = predictors_train_new, rug = TRUE, 
           main = "Partial dependence plot by BMI") +
  theme(plot.title = element_text(hjust = 0.5))

pdp::partial(gbmA_model, pred.var = c("SBP","LDL"),
             chull = TRUE,
             grid.resolution = 10) %>% 
  autoplot(train = predictors_train_new, rug = TRUE,legend.title = "Probability of long recovery time in Logit",
           main = "Partial dependence plot by SBP and LDL") +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r}
gbmA_model %>%
  pdp::partial(pred.var = "bmi",
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = predictors_train_new, alpha = .1) +
  ggtitle("ICE plot for bmi versus probability of long recovery (>30) in logit") +
  theme(plot.title = element_text(hjust = 0.5))
```

